import os
import time
import json
import textwrap
import numpy as np
import requests
from pathlib import Path
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class ModelAdapter:
    """é€šç”¨å¤§æ¨¡å‹é€‚é…å™¨ï¼Œæ”¯æŒå¤šç§å¤§æ¨¡å‹æ— ç¼åˆ‡æ¢"""

    def __init__(self, model_name="openai"):
        """
        åˆå§‹åŒ–æ¨¡å‹é€‚é…å™¨

        å‚æ•°:
            model_name: æ¨¡å‹åç§°
                å¯é€‰: "openai", "deepseek", "silicon", "qwen"
        """
        self.model_name = model_name
        self.setup_model()

    def setup_model(self):
        """æ ¹æ®æ¨¡å‹åç§°è®¾ç½®å¯¹åº”çš„æ¨¡å‹APIå‚æ•°"""
        # æ¨¡å‹æ”¯æŒçš„ç«¯ç‚¹é…ç½®
        self.model_config = {
            "openai": {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "model": "gpt-4-turbo",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                    "Content-Type": "application/json"
                },
                "embedding_endpoint": "https://api.openai.com/v1/embeddings",
                "embedding_model": "text-embedding-3-small"
            },
            "deepseek": {
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "endpoint": "https://api.deepseek.com/chat/completions",
                "model": "deepseek-chat",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('DEEPSEEK_API_KEY')}",
                    "Content-Type": "application/json"
                }
            },
            "silicon": {
                "api_key": os.getenv("SILICON_API_KEY"),
                "endpoint": "https://api.siliconflow.cn/v1/chat/completions",
                "model": "silicon-llama3",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('SILICON_API_KEY')}",
                    "Content-Type": "application/json"
                }
            },
            "qwen": {
                "api_key": os.getenv("QWEN_API_KEY"),
                "endpoint": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                "model": "qwen-turbo",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('QWEN_API_KEY')}",
                    "Content-Type": "application/json",
                    "X-DashScope-Async": "enable"
                }
            }
        }

        # éªŒè¯é…ç½®
        config = self.model_config.get(self.model_name)
        if not config:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self.model_name}")

        # ç¡®ä¿æœ‰APIå¯†é’¥
        if not config.get("api_key"):
            raise ValueError(f"æœªè®¾ç½® {self.model_name.upper()}_API_KEY ç¯å¢ƒå˜é‡")

    def generate(self, messages, max_tokens=1000, temperature=0.7):
        """ä½¿ç”¨é€‰å®šæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        config = self.model_config[self.model_name]

        # æ ¹æ®æ¨¡å‹æ„å»ºè¯·æ±‚ä½“
        payload = {
            "openai": {
                "model": config["model"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            "deepseek": {
                "model": config["model"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            "silicon": {
                "model": config["model"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            "qwen": {
                "model": config["model"],
                "input": {
                    "messages": messages
                },
                "parameters": {
                    "result_format": "message",
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            }
        }[self.model_name]

        try:
            response = requests.post(
                config["endpoint"],
                headers=config["headers"],
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            result = response.json()

            # è§£æä¸åŒæ¨¡å‹çš„å“åº”æ ¼å¼
            if self.model_name == "openai" or self.model_name == "deepseek" or self.model_name == "silicon":
                return result["choices"][0]["message"]["content"]
            elif self.model_name == "qwen":
                return result["output"]["text"]
            else:
                return result["choices"][0]["text"]

        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹è°ƒç”¨å¤±è´¥ ({self.model_name}): {str(e)}")

    def embed(self, text):
        """æ–‡æœ¬å‘é‡åŒ–ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½éœ€è¦ä¸“é—¨çš„åµŒå…¥APIï¼‰"""
        # å¯¹äºæ”¯æŒåµŒå…¥APIçš„æ¨¡å‹
        if self.model_name == "openai":
            config = self.model_config[self.model_name]
            response = requests.post(
                config["embedding_endpoint"],
                headers=config["headers"],
                json={
                    "input": text,
                    "model": config["embedding_model"]
                },
                timeout=60
            )
            response.raise_for_status()
            return response.json()["data"][0]["embedding"]
        else:
            # ç®€åŒ–ç‰ˆå‘é‡ç”Ÿæˆï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“ç”¨æ¨¡å‹ï¼‰
            np.random.seed(hash(text) % (2 ** 32))
            return np.random.rand(384).tolist()


class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨"""

    def __init__(self, data_path):
        self.data_path = Path(data_path)
        self.loaders = {
            ".pdf": PyPDFLoader,
            ".txt": TextLoader,
            ".md": TextLoader
        }

    def load(self, chunk_size=1000, chunk_overlap=200):
        """åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£"""
        documents = []

        for ext, loader_class in self.loaders.items():
            for file_path in self.data_path.rglob(f"*{ext}"):
                try:
                    print(f"åŠ è½½æ–‡æ¡£: {file_path.name}")
                    loader = loader_class(str(file_path))
                    docs = loader.load()

                    # æ·»åŠ å…ƒæ•°æ®
                    for doc in docs:
                        doc.metadata["source"] = str(file_path)
                        if ext == ".pdf":
                            # è°ƒæ•´é¡µç ç´¢å¼•
                            page = doc.metadata.get("page", 0)
                            doc.metadata["page"] = page + 1

                    documents.extend(docs)
                except Exception as e:
                    print(f"åŠ è½½å¤±è´¥ {file_path}: {e}")

        if not documents:
            return []

        # åˆ†å‰²æ–‡æ¡£
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        return splitter.split_documents(documents)


class RAGSystem:
    """é€šç”¨RAGç³»ç»Ÿï¼Œæ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢"""

    def __init__(self, data_path=None, llm_model="openai", embedding_model="openai"):
        """
        åˆå§‹åŒ– RAG ç³»ç»Ÿ

        å‚æ•°:
            data_path: çŸ¥è¯†åº“ç›®å½•è·¯å¾„
            llm_model: è¯­è¨€æ¨¡å‹ (openai, deepseek, silicon, qwen)
            embedding_model: åµŒå…¥æ¨¡å‹ (ç›®å‰ä»… openai æ”¯æŒä¸“ç”¨åµŒå…¥API)
        """
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm_adapter = ModelAdapter(llm_model)
        self.embedding_adapter = ModelAdapter(embedding_model)

        # å‘é‡å­˜å‚¨ç³»ç»Ÿ
        self.vector_store = None
        self.retriever = None

        # RAGé“¾
        self.rag_chain = None

        # æç¤ºæ¨¡æ¿
        self.prompt_template = ChatPromptTemplate.from_template(
            """è¯·ä½ ä½œä¸ºä¸“ä¸šåŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹æä¾›çš„å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            å¦‚æœå‚è€ƒä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶å»ºè®®å…¶ä»–èµ„æºã€‚
            å›ç­”è¦ç®€æ´å‡†ç¡®ï¼Œä½¿ç”¨ä¸­æ–‡ã€‚

            å‚è€ƒä¿¡æ¯:
            {context}

            é—®é¢˜: {question}

            å›ç­”:"""
        )

        # åˆå§‹åŒ–çŸ¥è¯†åº“
        if data_path:
            self.load_knowledge_base(data_path)

    def load_knowledge_base(self, data_path):
        """åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
        start_time = time.time()
        print(f"æ­£åœ¨åŠ è½½çŸ¥è¯†åº“: {data_path}")

        # 1. åŠ è½½æ–‡æ¡£
        loader = DocumentLoader(data_path)
        chunks = loader.load()

        if not chunks:
            print("è­¦å‘Š: æœªåŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œå°†ä½¿ç”¨åŸºç¡€é—®ç­”æ¨¡å¼")
            return

        print(f"å·²åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # 2. ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¯èƒ½ä½¿ç”¨ä¸åŒæ¨¡å‹çš„åµŒå…¥APIï¼‰
        embeddings = []
        for i, chunk in enumerate(chunks):
            try:
                embedding = self.embedding_adapter.embed(chunk.page_content)
                embeddings.append(embedding)
            except Exception as e:
                print(f"æ–‡æœ¬å— #{i + 1} å‘é‡åŒ–å¤±è´¥: {e}")
                # ä½¿ç”¨éšæœºå‘é‡æ›¿ä»£
                np.random.seed(hash(chunk.page_content) % (2 ** 32))
                embeddings.append(np.random.rand(384).tolist())

        # 3. åˆ›å»ºå‘é‡å­˜å‚¨
        try:
            self.vector_store = FAISS.from_embeddings(
                text_embeddings=list(zip(
                    [chunk.page_content for chunk in chunks],
                    embeddings
                )),
                embedding=self.embedding_adapter.embed,  # ç”¨äºæ–°æŸ¥è¯¢
                metadatas=[chunk.metadata for chunk in chunks]
            )
            self.retriever = self.vector_store.as_retriever(
                search_kwargs={"k": 3}  # è¿”å›3ä¸ªæœ€ç›¸å…³ç»“æœ
            )
            print("å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")

            # 4. æ„å»º RAG é“¾
            self.rag_chain = (
                    {"context": self.retriever, "question": RunnablePassthrough()}
                    | self.prompt_template
                    | (lambda x: self.run_llm(x))  # ä½¿ç”¨é€‚é…å™¨è°ƒç”¨LLM
                    | StrOutputParser()
            )
        except Exception as e:
            print(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
            print("é€€å›åˆ°åŸºæœ¬LLMé—®ç­”æ¨¡å¼")
            self.rag_chain = (
                    self.prompt_template.partial(context="")
                    | (lambda x: self.run_llm(x))
                    | StrOutputParser()
            )

        duration = time.time() - start_time
        print(f"åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)")

    def run_llm(self, prompt_dict):
        """è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤"""
        # æ„å»ºLLMè¯·æ±‚æ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user",
             "content": prompt_dict["messages"] if "messages" in prompt_dict else prompt_dict.to_string()}
        ]

        # è°ƒç”¨é€‚é…å™¨ç”Ÿæˆå›å¤
        return self.llm_adapter.generate(messages, max_tokens=1000)

    def query(self, question):
        """æŸ¥è¯¢ RAG ç³»ç»Ÿ"""
        if not self.rag_chain:
            return {
                "answer": "çŸ¥è¯†åº“å°šæœªåŠ è½½å®Œæˆ",
                "sources": []
            }

        # 1. è·å–å›ç­”
        start_time = time.time()
        try:
            answer = self.rag_chain.invoke(question)
        except Exception as e:
            answer = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        llm_time = time.time() - start_time

        # 2. è·å–æºæ–‡æ¡£
        sources = []
        if self.retriever:
            try:
                source_docs = self.retriever.invoke(question)
                sources = self._format_sources(source_docs)
            except Exception:
                pass

        # 3. è¿”å›æ ¼å¼åŒ–ç»“æœ
        return {
            "answer": answer,
            "sources": sources,
            "llm_time": llm_time,
            "llm_model": self.llm_adapter.model_name
        }

    def _format_sources(self, documents):
        """æ ¼å¼åŒ–æºæ–‡æ¡£ä¿¡æ¯"""
        sources = []
        for doc in documents:
            metadata = doc.metadata
            if isinstance(doc, dict):
                content = doc.get("page_content", "")
                metadata = doc.get("metadata", {})
            else:
                content = doc.page_content
                metadata = doc.metadata

            source_path = Path(metadata.get("source", "æœªçŸ¥æ–‡æ¡£"))
            sources.append({
                "file": source_path.name,
                "page": metadata.get("page", "æœªçŸ¥é¡µç "),
                "extract": textwrap.shorten(content, width=200, placeholder="...")
            })
        return sources


def create_sample_data(path):
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    path = Path(path)
    path.mkdir(exist_ok=True, parents=True)

    samples = [
        ("äººå·¥æ™ºèƒ½åŸºç¡€.md", """
        ## äººå·¥æ™ºèƒ½æ¦‚è¿°

        äººå·¥æ™ºèƒ½(Artificial Intelligence, AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
        æ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

        ## æ ¸å¿ƒæŠ€æœ¯
        - æœºå™¨å­¦ä¹ 
        - æ·±åº¦å­¦ä¹ 
        - è‡ªç„¶è¯­è¨€å¤„ç†
        """),

        ("å¤§è¯­è¨€æ¨¡å‹.pdf", """
        å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ˜¯åŸºäºTransformeræ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
        è¿™äº›æ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯ä»¥ç”Ÿæˆç±»äººæ–‡æœ¬ã€ç¿»è¯‘è¯­è¨€ã€å›ç­”é—®é¢˜å’Œæ€»ç»“æ–‡æ¡£ã€‚

        ### çŸ¥åæ¨¡å‹
        - GPTç³»åˆ— (OpenAI)
        - LLaMA (Meta)
        - Gemini (Google)
        - Qwen (é˜¿é‡Œ)
        """)
    ]

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ï¼ˆæ¨¡æ‹ŸPDFåˆ›å»ºï¼‰
    for filename, content in samples:
        file_path = path / filename

        # å¯¹PDFæ–‡ä»¶ä½¿ç”¨ç‰¹æ®Šå¤„ç†
        if filename.endswith(".pdf"):
            # è¿™é‡Œå®é™…åº”è¯¥åˆ›å»ºçœŸå®PDFï¼Œä½†ä¸ºç®€åŒ–ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ›¿ä»£
            file_path = path / (filename.replace(".pdf", ".txt"))
            content = "[PDFå†…å®¹æ¨¡æ‹Ÿ]\n" + content

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content.strip())

    print(f"å·²åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“: {path}")


def print_banner():
    """æ‰“å°ç³»ç»Ÿæ¨ªå¹…"""
    print("\n" + "=" * 70)
    print(textwrap.dedent("""
      ____   __    __    ___   _        ______   _____    _____ 
     / __ \ / /   / /   /   | | |      / ____/  / ___/   / ___/ 
    / /_/ // /   / /   / /| | | |     / / __   \__ \    \__ \  
   / _, _// /___/ /___/ ___ | | |___ / /_/ /  ___/ /   ___/ /  
  /_/ |_|/_____/_____/_/  |_| |_____(_)____/  /____/   /____/   
    """))
    print("=" * 70)
    print("é€šç”¨RAGç³»ç»Ÿ - æ”¯æŒå¤šç§å¤§æ¨¡å‹æ— ç¼åˆ‡æ¢")
    print("æ”¯æŒçš„æ¨¡å‹: OpenAI, DeepSeek, ç¡…åŸºæµåŠ¨, é€šä¹‰åƒé—®")
    print("=" * 70)


if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
    knowledge_path = Path("knowledge_base")
    knowledge_path.mkdir(exist_ok=True)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    create_sample_data(knowledge_path)

    # æ‰“å°ç³»ç»Ÿæ¨ªå¹…
    print_banner()

    # æ¨¡å‹é€‰æ‹©èœå•
    available_models = ["openai", "deepseek", "silicon", "qwen"]
    print("\nè¯·é€‰æ‹©è¯­è¨€æ¨¡å‹:")
    for i, model in enumerate(available_models):
        print(f"{i + 1}. {model.capitalize()}")

    # ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    model_choice = int(input("\nè¯·è¾“å…¥æ¨¡å‹ç¼–å· (1-4): ")) - 1
    selected_model = available_models[model_choice] if 0 <= model_choice < len(available_models) else "openai"

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGSystem(
        data_path=knowledge_path,
        llm_model=selected_model
    )

    print(f"\nå·²é€‰æ‹© {selected_model.capitalize()} æ¨¡å‹ï¼Œç³»ç»Ÿå°±ç»ªï¼")

    # äº¤äº’å¼é—®ç­”
    while True:
        print("\n" + "=" * 70)
        question = input("è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰:\n> ").strip()

        if question.lower() == 'exit':
            break
        if not question:
            continue

        # æŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœ
        response = rag.query(question)

        # æ‰“å°ç»“æœ
        print("\n" + "-" * 70)
        print(f"ğŸ’¡ [{response['llm_model'].upper()}æ¨¡å‹å›ç­”]:")
        print(textwrap.fill(response["answer"], width=70))
        print(f"\nâ±ï¸ æ¨¡å‹å“åº”æ—¶é—´: {response['llm_time']:.2f}ç§’")

        # æ˜¾ç¤ºæºæ–‡æ¡£
        if response["sources"]:
            print("\nğŸ” å‚è€ƒæ¥æº:")
            for i, source in enumerate(response["sources"]):
                print(f"{i + 1}. [{source['file']}] (é¡µ: {source['page']})")
                print(f"   {source['extract']}")
        else:
            print("\nâš ï¸ æœ¬æ¬¡å›ç­”æœªæ£€ç´¢çŸ¥è¯†åº“å†…å®¹")
        print("-" * 70)

    print("\næ„Ÿè°¢ä½¿ç”¨é€šç”¨RAGç³»ç»Ÿï¼Œå†è§ï¼")